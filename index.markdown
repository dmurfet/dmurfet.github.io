---
# Feel free to add content and custom Front Matter to this file.
# To modify the layout, see https://jekyllrb.com/docs/themes/#overriding-theme-defaults

layout: home
---

![Great ocean road](ocean.jpg?w=1740&h=980&fit=crop&crop=center&auto=format)

We work on deep learning, from fundamental mathematical theory through to real-world applications. 

There is a new industrial revolution on the way, and it is imperative that Australia increases the growth-rate of research and applied expertise in deep learning. To this end we aim to publish research in the top conferences (e.g. [NeurIPS](https://nips.cc/), [ICML](https://icml.cc/), [ICLR](https://iclr.cc/)) and train Masters and PhD students in this quickly emerging field. We hope that some of these students will found companies, providing employment for other mathematicians and contributing to Australian productivity growth through new forms of perceptual and cognitive automation.

We are part of the [School of Mathematics and Statistics](https://ms.unimelb.edu.au/home) at the University of Melbourne. We run a [seminar](http://therisingsea.org/post/seminar-ch/) on deep learning. We are looking for highly-motivated students to join our group, at either Masters or PhD level (see the Projects section below). You can be interested in anything from the engineering aspect of deep learning, all the way through to the algebraic geometry and statistics of neural networks.

Curious? Our [Discord](https://discord.gg/XwAc3hH) is a deep learning study group, all welcome.

<!--Feel free to drop by for a chat in our **public office hours** on [Zoom](https://unimelb.zoom.us/j/537135126) (on hiatus for one week while a new time is decided, see Discord).-->

<!--*Iluka is an Aboriginal Australian word meaning: [by the sea](https://www.gnb.nsw.gov.au/place_naming/placename_search/extract?id=MackXtrXan).*-->

## People

The group involves five faculty from across the School of Mathematics and Statistics. The primary researchers are those for whom deep learning is a major component of their overall research agenda:

* **[Mingming Gong](https://mingming-gong.github.io/)**: causal representation learning, transfer learning, trustworthy learning, computer vision. The recipient of a [2021 Discovery Early Career Researcher Award](https://dataportal.arc.gov.au/RGS/Web/Grants/DE210101624) to study learning causal graphs from unstructured data using deep learning.

* **[Susan Wei](https://www.suswei.com/)**: statistics, reinforcement learning, singular learning theory. The recipient of a [2020 Discovery Early Career Researcher Award](https://dataportal.arc.gov.au/NCGP/Web/Grant/Grant/DE200101253) to study fairness in deep learning.

* **[Daniel Murfet](http://therisingsea.org/)**: algebraic geometry, logic, deep reinforcement learning, singular learning theory. Deep reinforcement learning paper: [ICLR 2020](https://openreview.net/forum?id=rkecJ6VFvr) and papers on linear logic: [1](https://arxiv.org/abs/1407.2650) [2](https://arxiv.org/abs/1805.10770) [3](https://arxiv.org/abs/1805.11813).

* **[Feng Liu](https://fengliu90.github.io/)**: Hypothesis testing via deep learning, transfer learning, and trustworthy machine learning: adversarial data detection/generalization, out-of-distribution data detection/generalization, and privacy-leakage detection/protection of deep learning technologies. 

* **[Liam Hodgkinson](https://www.liamhodgkinson.com)**: probabilistic machine learning, deep learning theory, stochastic optimisation, robust neural network architectures, and data augmentation.


The other researchers in the group, for whom deep learning is a minor research area:

* **[Jesse Gell-Redman](https://sites.google.com/site/jessegellredman/)**: analysis, singular learning theory.

* **[Thomas Quella](https://researchers.ms.unimelb.edu.au/~tquella@unimelb/#home)**: mathematical physics, statistical mechanics, singular learning theory.

Our chief composer is [Lucas Cantor](https://www.lucascantormusic.com/) and our favourite piece of music is his [Softbank Sinfonia](https://open.spotify.com/album/3Y0xYCNHtl6HNeEoxyP96c?si=5VHQ4D54RauyuPIO4rx6Jg) (now officially released!).

### Students

Our PhD students:

* **Edmund Lau Tiew Hong**: singular learning theory, algebraic geometry.
* **Archer Moore**: causal representation learning, deep generative models
* **Erdun Gao**: causal representation learning, transfer learning
* **Dongting Hu**: depth estimation, neural radiance fields
* **Ziye Chen**: 3D object detection/segmentation, neural architecture search, neural network compression

Our MSc sudents:

* **Spencer Wong**: singular learning theory, algebraic geometry.
* **[Rohan Hitchcock](http://rohanhitchcock.com/)**: singular learning theory, algebraic geometry.
* **Liam Carroll**: singular learning theory, phase transitions and thermodynamics.
* **Matt Farrugia-Roberts**
* **Mark Drvodelic**: graph neural networks, bioscience
* **Qianjun Ding**: 3D human pose estimation
* **Kuoyuan Li**: neural 3D portraits 

## Research projects

Students affiliated with the group are primarily supervised by one of Gong, Wei, or Murfet and are expected to participate in the group seminar. We supervise students at both Masters and PhD level. Here are some of the currently active projects for which we are seeking student contributors:

* **Singular learning theory**: (led by [Susan Wei](https://www.suswei.com/), [Daniel Murfet](http://therisingsea.org/), [Jesse Gell-Redman](https://sites.google.com/site/jessegellredman/), [Thomas Quella](https://researchers.ms.unimelb.edu.au/~tquella@unimelb/#home), and [Liam Hodgkinson](https://www.liamhodgkinson.com)) Applications of algebraic geometry and stochastic processes to the development of a foundational theory of deep learning, following the work of [Sumio Watanabe](http://watanabe-www.math.dis.titech.ac.jp/users/swatanab/singular-learning-theory.html).

* **Fairness in deep learning**: (led by [Susan Wei](https://www.suswei.com/) and [Mingming Gong](https://mingming-gong.github.io/)) develop and implement statistical methods to fight against algorithm bias, by improving techniques for imposing invariance on deep learning algorithms.

* **Reasoning in deep reinforcement learning**: (led by [Daniel Murfet](http://therisingsea.org/)) in follow-up work to the [simplicial Transformer](https://openreview.net/forum?id=rkecJ6VFvr) we are applying these methods to the study of error correcting codes in the design of topological quantum computers, along the lines of [Sweke et al](https://arxiv.org/abs/1810.07207) (joint with James Wallbridge and James Clift). There are a variety of other possible projects in the context of deep reinforcement learning and Transformer architectures for scientific applications.

* **Implicit regularization**: (led by [Liam Hodgkinson](https://www.liamhodgkinson.com)) explaining real-world performance of deep learning models by developing examining their intrinsic capacity to self-regularise during training.

* **Causal representation learning**: (led by [Mingming Gong](https://mingming-gong.github.io/)) develop deep learning methods to infer causal graphs from unstructured data, such as images and text, and make use of the learned graphs for causal reasoning and decision making.

* **Deep hypothesis testing**: (led by [Feng Liu](https://fengliu90.github.io/)): develop deep learning based kernel hypothesis testing methods: including goodness-of-fit testing, two-sample testing, and independent testing. The developed methods can be applied to various data types, such as images.

* **Stochastic optimisation theory**: (led by [Liam Hodgkinson](https://www.liamhodgkinson.com)) examining and comparing behaviours of stochastic optimisers, including stochastic gradient descent, and their impact on performance when applied to train deep learning models. 

* **Program synthesis in linear logic**: (led by [Daniel Murfet](http://therisingsea.org/)) building on a series of [recent](https://arxiv.org/abs/1805.10770) [papers](https://arxiv.org/abs/1805.11813) with James Clift we are using differential linear logic to lay the foundations for a theory of gradient-based program synthesis ([survey](https://gist.github.com/dmurfet/688af9d4413cbb9a13ca5d50b28ddcbc)), also in the context of singular learning theory. This project involves logic as well as implementation in Tensorflow or PyTorch. These topics are discussed in [a recent talk](https://youtu.be/IW4LjjAWrO4).

* **Deep transfer learning**: (led by [Mingming Gong](https://mingming-gong.github.io/) and [Feng Liu](https://fengliu90.github.io/)) leverage causal knowledge to develop deep learning models that can generalize/adapt to test data with distributions different from the training distribution, with applications to computer vision.

* **Trustworthy machine learning**: (led by [Feng Liu](https://fengliu90.github.io/), [Liam Hodgkinson](https://www.liamhodgkinson.com), and [Mingming Gong](https://mingming-gong.github.io/)): develop deep learning algorithm to train trustworthy models in complex and imperfect environments; make deep models be trusted when facing adversarial attacks, out-of-distribution data, and privacy attacks.

* **Deep generative models**: (led by [Mingming Gong](https://mingming-gong.github.io/) and [Liam Hodgkinson](https://www.liamhodgkinson.com)) leverage the power of neural networks to learn a function which can approximate the model distribution to the true distribution, with applications to image generation and editing.

* **3D computer vision**: (led by [Mingming Gong](https://mingming-gong.github.io/), [Feng Liu](https://fengliu90.github.io/), and [Liam Hodgkinson](https://www.liamhodgkinson.com)) develop deep learning methods to understand the geometry and depth of 3D scenes from 2D images.

The required background for these projects varies widely. In the more engineering-led projects you should already be a highly competent programmer and some kind of coding test may be part of the application process. For the more theory-led projects we are looking for students with a strong pure math background and basic programming skills (and the willingness to quickly develop those skills).

To apply send an email to one of the primary supervisors [Gong](mailto:mingming.gong@unimelb.edu.au), [Wei](mailto:susan.wei@unimelb.edu.au), [Murfet](mailto:d.murfet@unimelb.edu.au), [Liu](mailto:d.feng.liu1@unimelb.edu.au) or [Hodgkinson](lhodgkinson@unimelb.edu.au) with your CV and transcript (note the official process is no different to a normal Masters or PhD application, in particular we do not currently have any extraordinary scholarships to offer).

## Events

We run a research seminar on a range of topics within deep learning, *in hiatus for semester one of 2020*. For past seminars see [here](seminar). The best way to be notified of upcoming deep learning classes, bootcamps or seminars run by the group is to subscribe to the [group mailing list](https://tinyletter.com/mdlg). 
