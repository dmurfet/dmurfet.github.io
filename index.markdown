---
# Feel free to add content and custom Front Matter to this file.
# To modify the layout, see https://jekyllrb.com/docs/themes/#overriding-theme-defaults

layout: home
---

![Great ocean road](https://outbackfree.imgix.net/content/tours/Grayline/Great_Ocean_Road/064OK_IMGP0415-X2.jpg?w=1740&h=980&fit=crop&crop=center&auto=format)

We work on deep learning, from fundamental mathematical theory through to real-world applications. 

There is a new industrial revolution on the way, and it is imperative that Australia increases the growth-rate of research and applied expertise in deep learning. To this end we aim to publish research in the top conferences (e.g. [NeurIPS](https://nips.cc/), [ICML](https://icml.cc/), [ICLR](https://iclr.cc/)) and train Masters and PhD students in this quickly emerging field. We hope that some of these students will found companies, providing employment for other mathematicians and contributing to Australian productivity growth through new forms of perceptual and cognitive automation.

We are part of the [School of Mathematics and Statistics](https://ms.unimelb.edu.au/home) at the University of Melbourne. We run a [seminar](http://therisingsea.org/post/seminar-ch/) on deep learning and maintain a community resource about [deep learning in Australia](dlinoz). We are looking for highly-motivated students to join our group, at either Masters or PhD level: [apply here](mailto:d.murfet@unimelb.edu.au). You can be interested in anything from the engineering aspect of deep learning, all the way through to the algebraic geometry and statistics of neural networks.

*Iluka is an Aboriginal Australian word meaning: [by the sea](https://www.gnb.nsw.gov.au/place_naming/placename_search/extract?id=MackXtrXan).*

## People

The lab involves five faculty from across the School of Mathematics and Statistics. The primary researchers are those for whom deep learning is a major component of their overall research agenda:

* [Mingming Gong](https://mingming-gong.github.io/): causal discovery, transfer learning, deep learning. Some relevant papers: [NeurIPS 2019](https://arxiv.org/abs/1907.02690), [NeurIPS 2019](https://papers.nips.cc/paper/8912-likelihood-free-overcomplete-ica-and-applications-in-causal-discovery.pdf), [NeurIPS 2019](https://papers.nips.cc/paper/9506-specific-and-shared-causal-relation-modeling-and-mechanism-based-clustering.pdf), [ICML 2019](http://proceedings.mlr.press/v97/huang19g/huang19g.pdf), [CVPR 2019](https://arxiv.org/abs/1809.05852), [CVPR 2019](https://arxiv.org/pdf/1904.01870.pdf).
* Susan Wei: statistics, reinforcement learning, singular learning theory. The recipient of a [2020 Discovery Early Career Researcher Award](https://dataportal.arc.gov.au/NCGP/Web/Grant/Grant/DE200101253) to study fairness in deep learning.
* [Daniel Murfet](http://therisingsea.org/): algebraic geometry, logic, deep reinforcement learning, singular learning theory. Deep reinforcement learning paper: [ICLR 2020](https://openreview.net/forum?id=rkecJ6VFvr) and papers on linear logic: [1](https://arxiv.org/abs/1805.10770) [2](https://arxiv.org/abs/1805.11813).

The other researchers in the lab, for whom deep learning is a minor research area:

* [Jesse Gell-Redman](https://sites.google.com/site/jessegellredman/): analysis, singular learning theory.
* [Thomas Quella](https://researchers.ms.unimelb.edu.au/~tquella@unimelb/#home): mathematical physics, statistical mechanics, singular learning theory.

## Research projects

Students affiliated with the lab have a primary supervisor (one of Gong, Wei, Murfet) and a co-supervisor, and are expected to participate in the lab seminar as well as contributing to a research project. Generally speaking we only supervise students at Masters and PhD level, but exceptional undergraduates may also apply. Here are some of the currently active projects for which we are seeking student contributors:

* **Generative Adversarial networks**: (led by Mingming Gong) *details coming*

* **Bias in deep learning:** (led by Susan Wei) *details coming*

* **Singular learning theory:** (led by Susan Wei, Daniel Murfet) Applications of algebraic geometry and stochastic processes to the development of a foundational theory of deep learning.

* **Reasoning in deep reinforcement learning:** (led by Daniel Murfet) in follow-up work to the [simplicial Transformer](https://openreview.net/forum?id=rkecJ6VFvr) we are applying these methods to the study of error correcting codes in the design of topological quantum computers, along the lines of [Sweke et al](https://arxiv.org/abs/1810.07207). There are a variety of other possible projects in the context of deep reinforcement learning and Transformer architectures for scientific applications.

* **Program synthesis in linear logic**: (led by Daniel Murfet) building on a series of [recent](https://arxiv.org/abs/1805.10770) [papers](https://arxiv.org/abs/1805.11813) we are using differential linear logic to lay the foundations a theory of gradient-based [program synthesis](https://gist.github.com/dmurfet/688af9d4413cbb9a13ca5d50b28ddcbc), also in the context of singular learning theory. This project would involve learning background logic, as well as implementations of logical ideas in Tensorflow or PyTorch.

The required background for these projects varies widely. In the more engineering-led projects you should already be a highly competent programmer and some kind of coding test may be part of the application process. For the more theory-led projects we are looking for students with a strong pure math background and basic programming skills (and the willingness to quickly develop those skills).

## Events

### Classes

In semester two of 2020 we will be running deep learning bootcamps, and possibly other kinds of classes. **TBA**.

### Seminar

We run a research seminar on a range of topics within deep learning, *currently in hiatus*. For past seminars see [here](seminar).
